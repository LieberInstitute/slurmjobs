---
title: "Introduction to slurmjobs"
author: 
  - name: Nicholas J. Eagles
    affiliation:
    - &libd Lieber Institute for Brain Development
    email: nickeagles77@gmail.com
  - name: Leonardo Collado-Torres
    affiliation:
    - *libd
    - &ccb Center for Computational Biology, Johns Hopkins University
    email: lcolladotor@gmail.com
output: 
  BiocStyle::html_document:
    self_contained: yes
    toc: true
    toc_float: true
    toc_depth: 2
    code_folding: show
date: "`r doc_date()`"
package: "`r pkg_ver('slurmjobs')`"
vignette: >
  %\VignetteIndexEntry{Introduction to slurmjobs}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}  
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
    collapse = TRUE,
    comment = "#>",
    crop = NULL ## Related to https://stat.ethz.ch/pipermail/bioc-devel/2020-April/016656.html
)
```


```{r vignetteSetup, echo=FALSE, message=FALSE, warning = FALSE}
## Track time spent on making the vignette
startTime <- Sys.time()

## Bib setup
library("RefManageR")
library("knitcitations")

## Write bibliography information
bib <- c(
    dplyr = citation("dplyr")[1],
    R = citation(),
    BiocStyle = citation("BiocStyle")[1],
    knitcitations = citation("knitcitations"),
    knitr = citation("knitr")[1],
    RefManageR = citation("RefManageR")[1],
    rmarkdown = citation("rmarkdown")[1],
    sessioninfo = citation("sessioninfo")[1],
    testthat = citation("testthat")[1],
    slurmjobs = citation("slurmjobs")[1]
)
```

# Basics

## Install `slurmjobs`

`R` is an open-source statistical environment which can be easily modified to enhance its functionality via packages. `r Biocpkg("slurmjobs")` is a `R` package available via the [Bioconductor](http://bioconductor.org) repository for packages. `R` can be installed on any operating system from [CRAN](https://cran.r-project.org/) after which you can install `r Biocpkg("slurmjobs")` by using the following commands in your `R` session:

```{r "install", eval = FALSE}
if (!requireNamespace("BiocManager", quietly = TRUE)) {
    install.packages("BiocManager")
}

BiocManager::install("slurmjobs")

## Check that you have a valid Bioconductor installation
BiocManager::valid()
```

## Required knowledge

`r Biocpkg("slurmjobs")` is designed for interacting with the [SLURM](https://slurm.schedmd.com/) job scheduler, and assumes basic familiarity with terms like "job", "task", and "array", as well as the [sbatch](https://slurm.schedmd.com/sbatch.html) command. Background knowledge about memory (such as virtual memory and resident set size (RSS)) is helpful but not critical in using this package.

If you are asking yourself the question "Where do I start using Bioconductor?" you might be interested in [this blog post](http://lcolladotor.github.io/2014/10/16/startbioc/#.VkOKbq6rRuU).

## Asking for help

As package developers, we try to explain clearly how to use our packages and in which order to use the functions. But `R` and `Bioconductor` have a steep learning curve so it is critical to learn where to ask for help. The blog post quoted above mentions some but we would like to highlight the [Bioconductor support site](https://support.bioconductor.org/) as the main resource for getting help: remember to use the `slurmjobs` tag and check [the older posts](https://support.bioconductor.org/tag/slurmjobs/). Other alternatives are available such as creating GitHub issues and tweeting. However, please note that if you want to receive help you should adhere to the [posting guidelines](http://www.bioconductor.org/help/support/posting-guide/). It is particularly critical that you provide a small reproducible example and your session information so package developers can track down the source of the error.

## Citing `slurmjobs`

We hope that `r Biocpkg("slurmjobs")` will be useful for your research. Please use the following information to cite the package and the overall approach. Thank you!

```{r "citation"}
## Citation info
citation("slurmjobs")
```

# Overview

`slurmjobs` provides helper functions for interacting with [SLURM](https://slurm.schedmd.com/)-managed high-performance-computing environments from R. It includes functions for creating submittable jobs (including array jobs), monitoring partitions, and extracting info about running or complete jobs. In addition to loading `slurmjobs`, we'll be using `dplyr` to manipulate example data about jobs.

```{r "start", message=FALSE}
library("slurmjobs")
library("dplyr")
```

# Creating Shell Scripts to `sbatch`

When processing data on a [SLURM](https://slurm.schedmd.com/)-managed system, primarily
running R code, you'll likely find yourself writing many "wrapper" shell scripts that can be
submitted via `sbatch` to the job scheduler. This process requires precise SLURM-specific
syntax and a large amount of repetition. `job_single` aims to reduce required configuration
from the user to just a handful of options that tend to vary most often between shell scripts
(e.g. memory, number of CPUs, time limit), and automate the rest of the shell-script-creation process.

Shell scripts created by `job_single` log key reproducibility information, such as the user,
job ID, job name, node name, and when the job starts and ends.

```{r "job_single_basic"}
#   With 'create_shell = FALSE', the contents of the potential shell script are
#   only printed to the screen
job_single(
    name = "my_shell_script", memory = "10G", cores = 2, create_shell = FALSE
)
```

Similarly, we can specify `task_num` to create an array job-- in this case, one with 10 tasks.

```{r "job_single_array"}
job_single(
    name = "my_array_job", memory = "5G", cores = 1, create_shell = FALSE,
    task_num = 10
)
```

Another function, `job_loop()`, can be used to create more complex array jobs as compared with
`job_single()`. It's useful when looping through one or more variables with pre-defined
values, and applying the same processing steps. The key difference is that rather than
specifying `task_num`, you specify `loops`, a named list of variables to loop through. An
array job then gets created that can directly refer to the values of these variables, rather
than referring to just the array's task ID.

`job_loop()`, unlike `job_single()`, also creates an R script. The idea is that the shell
script invokes the R script internally, with a particular combination of variables. The `getopt`
package is then used to read in this combination from the command line, so that each variable
can be accessed by name in R. Let's make that a bit more concrete.

```{r 'loop_shell'}
#   'job_loop' returns a list containing the contents of the to-be-created shell
#   and R scripts. Let's take a look at the shell script first
script_pair <- job_loop(
    loops = list(region = c("DLPFC", "HIPPO"), feature = c("gene", "exon", "tx", "jxn")),
    name = "bsp2_test"
)
cat(script_pair[["shell"]], sep = "\n")
```

First, note the line `Rscript bsp2_test.R --region ${region} --feature ${feature}`. Every task
of the array job passes a unique combination of `${region}` and `${feature}` to R.

Notice also that logs from executing this shell script get named with each of the variables'
values in addition to the array task ID. For example, the log for the first task would be
`logs/DLPFC_gene_1.txt`. Also, the array specifies 8 tasks total (the product of the number
of `region`s and `feature`s).

Let's also look at the R script.

```{r 'loop_r'}
cat(script_pair[["R"]], sep = "\n")
```

The code related to `getopt` at the top of the script reads in the unique combination of
variable values into a list called `opt` here. For example, one task of the array job might
yield values for `opt$region` and `opt$feature` to be `"DLPFC"` and `"gene"`, respectively.

# Submitting and Resubmitting Jobs

Shell scripts created with `job_single()` or `job_loop()` may be submitted as batch jobs with `sbatch` (e.g. `sbatch myscript.sh`). Note no additional arguments to `sbatch` are required since all configuration is specified within the shell script.

The `array_submit()` helper function was also intended to make job submission easier. In
particular, it addresses a common case where after a large array job was run, a handful of
tasks fail (such as due to temporary file-system issues). `array_submit()` helps re-submit
failed tasks.

Below we'll create an example array job with `job_single()`, then do a dry run of
`array_submit()` to demonstrate its basic usage.

```{r "array_submit_manual"}
job_single(
    name = "my_array_job", memory = "5G", cores = 1, create_shell = TRUE,
    task_num = 10
)

#   Suppose that tasks 3, 6, 7, and 8 failed
array_submit(name = "my_array_job", task_ids = c(3, 6:8), submit = FALSE)
```

While `task_ids` can be provided explicitly as above, the real convenience comes from the
ability to run `array_submit()` without specifying `task_ids`. As long as the original array
job was created with `job_single()` or `job_loop()` and submitted as-is (on the full set of
tasks), `array_submit()` can automatically find the failed tasks by reading the shell script
(`my_array_job.sh`), grabbing the original array job ID from the log, and internally calling
`job_report()`).

```{r "array_submit_auto", eval = FALSE}
#   Not run here, since we aren't on a SLURM cluster
array_submit(name = "my_array_job", submit = FALSE)
```

# Monitoring Running Jobs

The `job_info()` function provides wrappers around the `squeue` and `sstat` utilities SLURM provides for monitoring specific jobs and how busy partitions are. The general idea is to provide the information output from `squeue` into a `tibble`, while retrieving memory-utilization information that ordinarily must be retrieved manually on a job-by-job basis with `sstat -j [specific job ID]`.

On a SLURM system, you'd run `job_info_df = job_info(user = NULL, partition = "shared")` here, to get every user's jobs running on the "shared" partition. We'll load an example output directly here.

```{r "job_info_quick_look"}
#   On a real SLURM system
print(job_info_df)
```

The benefit to having this data in R, now, is to be able to trivially ask summarizing questions. First, "how much memory and how many CPUs am I currently using?" Knowing this answer can help ensure fair and civil use of shared computing resources, for example on a computing cluster.

```{r "job_info_total_resources"}
job_info_df |>
    #   Or your username here
    filter(user == "user21") |>
    #   Get the number of CPUs requested and the memory requested in GB
    summarize(
        total_mem_req = sum(requested_mem_gb),
        total_cpus = sum(cpus)
    ) |>
    print()
```

# Monitoring Partitions

Sometimes, it's useful to know about the partitions as a whole rather than about specific
jobs. `partition_info()` serves this purpose, and parses `sinfo` output into a `tibble`.
We'll load an example of the output from `partition_info(partition = NULL, all_nodes = FALSE)`.

```{r "partition_info_quick_look"}
print(partition_df)
```

Since `all_nodes` was `FALSE`, there's one row per partition, summarizing information across
all nodes that compose each partition. Alternatively, set `all_nodes` to `TRUE` to yield one
row per node.

With `partition_df`, let's summarize how busy the cluster is as a whole, then rank partitions
by amount of free memory.

```{r "partition_info_deep_dive"}
#   Print the proportion of CPUs and memory available for the whole cluster
partition_df |>
    summarize(
        prop_free_cpus = sum(free_cpus) / sum(total_cpus),
        prop_free_mem_gb = sum(free_mem_gb) / sum(total_mem_gb)
    ) |>
    print()

#   Now let's take the top 3 partitions by memory currently available
partition_df |>
    arrange(desc(free_mem_gb)) |>
    select(partition, free_mem_gb) |>
    slice_head(n = 3)
```

# Analyzing Finished Jobs

The `job_report()` function returns in-depth information about a single queued, running, or finished job (including a single array job). It combines functionality from SLURM's `sstat` and `sacct` to return a tibble for easy manipulation in R.

Suppose you have a workflow that operates as an array job, and you'd like to profile memory usage across the many tasks. Suppose we've done an initial trial, setting memory relatively high just to get the jobs running without issues. One use of `job_report` could be to determine a better memory request in a data-driven way-- the better settings can then be run on the larger dataset after the initial test.

On an actual system with SLURM installed, you'd normally run something like `job_df = job_report(slurm_job_id)` for the `slurm_job_id` (character or integer) representing the small test. For convenience, we'll start from the output of `job_report` as available in the `slurmjobs` package.

```{r "job_report_quick_look"}
job_df <- readRDS(
    system.file("extdata", "job_report_df.rds", package = "slurmjobs")
)
print(job_df)
```

Now let's choose a better memory request:

```{r "job_report_adjust_mem"}
stat_df <- job_df |>
    #   This example includes tasks that fail. We're only interested in memory
    #   for successfully completed tasks
    filter(status != "FAILED") |>
    summarize(
        mean_mem = mean(max_vmem_gb),
        std_mem = sd(max_vmem_gb),
        max_mem = max(max_vmem_gb)
    )

#   We could choose a new memory request as 3 standard deviations above the mean
#   of actual memory usage
new_limit <- stat_df$mean_mem + 3 * stat_df$std_mem

print(
    sprintf(
        "%.02fG is a better memory request than %.02fG, which was used before",
        new_limit,
        job_df$requested_mem_gb[1]
    )
)
```

# Reproducibility

The `r Biocpkg("slurmjobs")` package `r Citep(bib[["slurmjobs"]])` was made possible thanks to:

* R `r Citep(bib[["R"]])`
* `r Biocpkg("BiocStyle")` `r Citep(bib[["BiocStyle"]])`
* `r CRANpkg("dplyr")` `r Citep(bib[["dplyr"]])`
* `r CRANpkg("knitr")` `r Citep(bib[["knitr"]])`
* `r CRANpkg("RefManageR")` `r Citep(bib[["RefManageR"]])`
* `r CRANpkg("rmarkdown")` `r Citep(bib[["rmarkdown"]])`
* `r CRANpkg("sessioninfo")` `r Citep(bib[["sessioninfo"]])`
* `r CRANpkg("testthat")` `r Citep(bib[["testthat"]])`

This package was developed using `r BiocStyle::Biocpkg("biocthis")`.


Code for creating the vignette

```{r createVignette, eval=FALSE}
## Create the vignette
library("rmarkdown")
system.time(render("slurmjobs.Rmd", "BiocStyle::html_document"))

## Extract the R code
library("knitr")
knit("slurmjobs.Rmd", tangle = TRUE)
```

Date the vignette was generated.

```{r reproduce1, echo=FALSE}
## Date the vignette was generated
Sys.time()
```

Wallclock time spent generating the vignette.

```{r reproduce2, echo=FALSE}
## Processing time in seconds
totalTime <- diff(c(startTime, Sys.time()))
round(totalTime, digits = 3)
```

`R` session information.

```{r reproduce3, echo=FALSE}
## Session info
library("sessioninfo")
options(width = 120)
session_info()
```



# Bibliography

This vignette was generated using `r Biocpkg("BiocStyle")` `r Citep(bib[["BiocStyle"]])`
with `r CRANpkg("knitr")` `r Citep(bib[["knitr"]])` and `r CRANpkg("rmarkdown")` `r Citep(bib[["rmarkdown"]])` running behind the scenes.

Citations made with `r CRANpkg("RefManageR")` `r Citep(bib[["RefManageR"]])`.

```{r vignetteBiblio, results = "asis", echo = FALSE, warning = FALSE, message = FALSE}
## Print bibliography
PrintBibliography(bib, .opts = list(hyperlink = "to.doc", style = "html"))
```
